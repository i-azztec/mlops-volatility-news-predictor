#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –º–æ–¥–µ–ª–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏.
–≠–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Å–∏—Å—Ç–µ–º—ã –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–µ–¥–µ–ª—å —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–π –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–µ–π.
"""

import pandas as pd
import numpy as np
import psycopg2
from datetime import datetime, timedelta
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ PYTHONPATH
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# PostgreSQL connection
POSTGRES_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'monitoring',
    'user': 'user',
    'password': 'password'
}

def connect_to_db():
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
    return psycopg2.connect(**POSTGRES_CONFIG)

def insert_metric(cursor, timestamp, metric_name, metric_value):
    """–í—Å—Ç–∞–≤–ª—è–µ—Ç –æ–¥–Ω—É –º–µ—Ç—Ä–∏–∫—É –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    cursor.execute(
        "INSERT INTO volatility_metrics (timestamp, metric_name, metric_value) VALUES (%s, %s, %s)",
        (timestamp, metric_name, metric_value)
    )

def generate_degrading_metrics(base_accuracy=0.714, base_f1=0.750, base_auc=0.555, weeks=4):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–π –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏
    
    Args:
        base_accuracy: –ù–∞—á–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        base_f1: –ù–∞—á–∞–ª—å–Ω—ã–π F1-score
        base_auc: –ù–∞—á–∞–ª—å–Ω—ã–π AUC
        weeks: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        List of tuples (timestamp, metrics_dict)
    """
    results = []
    
    # –ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –∏ –∏–¥–µ–º –Ω–∞–∑–∞–¥
    current_date = datetime.now()
    
    for week in range(weeks):
        # –ö–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é –º–æ–¥–µ–ª—å –¥–µ–≥—Ä–∞–¥–∏—Ä—É–µ—Ç –Ω–∞ 1-3%
        degradation_factor = 1 - (week * 0.02 + np.random.normal(0, 0.01))
        degradation_factor = max(0.5, degradation_factor)  # –ù–µ –ø–∞–¥–∞–µ–º –Ω–∏–∂–µ 50%
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 3-4 –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤ –Ω–µ–¥–µ–ª—é
        measurements_per_week = np.random.randint(3, 5)
        
        for measurement in range(measurements_per_week):
            # –í—Ä–µ–º—è –∏–∑–º–µ—Ä–µ–Ω–∏—è (—Å–ª—É—á–∞–π–Ω–æ–µ –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ–¥–µ–ª–∏)
            days_back = week * 7 + measurement * (7 / measurements_per_week)
            timestamp = current_date - timedelta(days=days_back)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º –∫ –º–µ—Ç—Ä–∏–∫–∞–º
            noise = np.random.normal(0, 0.02)  # 2% —à—É–º
            
            metrics = {
                'accuracy': max(0.4, min(0.9, base_accuracy * degradation_factor + noise)),
                'f1': max(0.3, min(0.9, base_f1 * degradation_factor + noise * 0.8)),
                'auc': max(0.45, min(0.75, base_auc * degradation_factor + noise * 0.5)),
                
                # Data drift –º–µ—Ç—Ä–∏–∫–∏ (—É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
                'data_drift_detected': 1 if week > 2 and np.random.random() > 0.7 else 0,
                'drift_share': min(0.3, week * 0.05 + np.random.uniform(0, 0.05)),
                
                # –†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫ (–≤–∞—Ä—å–∏—Ä—É—é—Ç—Å—è)
                'current_samples': np.random.randint(150, 300),
                'reference_samples': 6300,  # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
                
                # –ö–≤–∞–Ω—Ç–∏–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                'prediction_mean_proba_quantile_0.05': np.random.uniform(0.1, 0.3),
                'prediction_mean_proba_quantile_0.95': np.random.uniform(0.7, 0.9),
            }
            
            results.append((timestamp, metrics))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º)
    results.sort(key=lambda x: x[0])
    return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞...")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ 6 –Ω–µ–¥–µ–ª—å
    historical_data = generate_degrading_metrics(weeks=6)
    
    print(f"üìä –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(historical_data)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫")
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    try:
        conn = connect_to_db()
        cursor = conn.cursor()
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        print("üóëÔ∏è –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        cursor.execute("DELETE FROM volatility_metrics WHERE timestamp < NOW() - INTERVAL '2 months'")
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        print("üíæ –í—Å—Ç–∞–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        total_inserted = 0
        
        for timestamp, metrics in historical_data:
            for metric_name, metric_value in metrics.items():
                insert_metric(cursor, timestamp, metric_name, metric_value)
                total_inserted += 1
        
        # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        conn.commit()
        print(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {total_inserted} –º–µ—Ç—Ä–∏–∫ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        cursor.execute("""
            SELECT 
                metric_name, 
                COUNT(*) as count,
                MIN(metric_value) as min_val,
                MAX(metric_value) as max_val,
                AVG(metric_value) as avg_val
            FROM volatility_metrics 
            WHERE timestamp > NOW() - INTERVAL '2 months'
            GROUP BY metric_name 
            ORDER BY metric_name
        """)
        
        print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫:")
        print("–ú–µ—Ç—Ä–∏–∫–∞\t\t\t\t–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ\t–ú–∏–Ω\t–ú–∞–∫—Å\t–°—Ä–µ–¥–Ω–µ–µ")
        print("-" * 70)
        
        for row in cursor.fetchall():
            metric_name, count, min_val, max_val, avg_val = row
            print(f"{metric_name:30s}\t{count:3d}\t{min_val:.3f}\t{max_val:.3f}\t{avg_val:.3f}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö: {e}")
        return 1
    
    finally:
        if 'conn' in locals():
            conn.close()
    
    print("\nüéØ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å Grafana –¥–∞—à–±–æ—Ä–¥!")
    print("   –î–æ—Å—Ç—É–ø: http://localhost:3000")
    print("   –õ–æ–≥–∏–Ω: admin / admin")
    
    return 0

if __name__ == "__main__":
    exit(main())
